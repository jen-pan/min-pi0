log_dir: /Users/jenny/train/${name}/${now:%Y-%m-%d}_${now:%H-%M}
name: ${data.train.dataset_mix}_action_${num_action_tokens}
n_nodes: 1
seed: 16
pretrained_model_path: /Users/jenny/train/tmp/paligemma-3b-pt-224
load_pretrained: True
resume_checkpoint_path: ${log_dir}/checkpoints/model.pkl
use_bf16: True
debug: False

wandb:
  entity: jenny/min-pi0
  project: min-pi0
  run: ${now:%H-%M-%S}_${name}

lr: 5e-5
lr_scheduler:
  first_cycle_steps: 10000000
  min_lr: 1e-8
  warmup_steps: 100
weight_decay: 0
max_grad_norm: 1.0
action_clip_value: 1.0

num_proprio_tokens: 1
num_action_tokens: 4 # 50 in the paper
num_inference_steps: 10

action_dim: 7 
proprio_dim: 7  
max_seq_len: 275  # fixed 256 for image + some for text
tokenizer_padding: max_length 
max_image_text_tokens: ${max_seq_len}
image_token_index: 257152
vocab_size: 257216
pad_token_id: 0

log_freq: 20
val_freq: 2000
save_model_freq: 2000
n_epochs: 16
n_updates: 32000
per_device_batch_size: 16

data:
  train:
    dataset_mix: oxe_simple 
    data_path: /Users/jenny/data/
    window_size: ${num_proprio_tokens}
    action_horizon: ${num_action_tokens}
    skip_unlabeled: True
    load_proprio: True
    shuffle_buffer_size: 100000
    num_parallel_calls: 100
    traj_transform_threads: 16
    traj_read_threads: 16
  val: 
    shuffle_buffer_size: 10000
  
siglip_model:
  config:
    hidden_size: 1152 
    intermediate_size: 4304
    num_hidden_layers: 27
    num_attention_heads: 16
    num_channels: 3
    image_size: 224
    patch_size: 14
    layer_norm_eps: 1e-6
    attention_dropout: 0.0
    num_image_tokens: 256

multimodal_projector:
  config:
    vision_config:
      hidden_size: 1152
      projection_dim: 2048

moe:
  config:
    multimodal: ${multimodal}
    num_hidden_layers: 18
    num_attention_heads: 8
    num_key_value_heads: 1
    head_dim: 256
    max_position_embeddings: 8192
    rms_norm_eps: 1e-6
    rope_theta: 10000.0
    attention_bias: False
    attention_dropout: 0.0
    pad_token_id: ${pad_token_id}

multimodal:
  image_text:   
    hidden_size: 2048
    intermediate_size: 16384
    use_final_norm: False
    cache: True
  proprio:
    hidden_size: 1024
    intermediate_size: 4096
    use_final_norm: True  # this gets zeroed out in the final layer
    cache: True
  action:
    hidden_size: 1024
    intermediate_size: 4096
    use_final_norm: True
    cache: False